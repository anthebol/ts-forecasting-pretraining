{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# class ARNonstationaryGenerator:\n",
    "#     \"\"\"Generator for non-stationary time series using AR process with varying weights\"\"\"\n",
    "\n",
    "#     def __init__(self, seq_length=900, num_samples=1000, seed=42, noise_variance=16):\n",
    "#         np.random.seed(seed)\n",
    "#         self.seq_length = seq_length\n",
    "#         self.num_samples = num_samples\n",
    "#         self.noise_variance = noise_variance\n",
    "\n",
    "#         # Default segment points and AR coefficients\n",
    "#         self.segment_points = [0, 300, 600, 900]\n",
    "#         self.ar_coefficients = [\n",
    "#             [0.8, -0.5],  # First segment (Stable)\n",
    "#             [0.6, -0.2],  # Second segment (Stable)\n",
    "#             [0.4, -0.3],  # Third segment (Stable)\n",
    "#         ]\n",
    "\n",
    "#     def generate_ar_sequence(self):\n",
    "#         \"\"\"\n",
    "#         Generate a non-stationary signal with varying AR coefficients.\n",
    "\n",
    "#         Returns:\n",
    "#             np.ndarray: Generated non-stationary signal.\n",
    "#         \"\"\"\n",
    "#         seq_length = self.seq_length\n",
    "#         # Initialize signal and input\n",
    "#         x = np.random.normal(0, 1, seq_length)\n",
    "#         q = np.random.normal(0, np.sqrt(self.noise_variance), seq_length)\n",
    "#         y = np.zeros(seq_length)\n",
    "\n",
    "#         # Generate signal for each segment\n",
    "#         for seg_idx in range(len(self.segment_points) - 1):\n",
    "#             start = self.segment_points[seg_idx]\n",
    "#             end = self.segment_points[seg_idx + 1]\n",
    "#             w = self.ar_coefficients[seg_idx]\n",
    "\n",
    "#             for n in range(start, end):\n",
    "#                 if n >= 2:\n",
    "#                     y[n] = w[0] * x[n] + w[1] * x[n - 1] + q[n]\n",
    "#                 elif n == 1:\n",
    "#                     y[n] = w[0] * x[n] + q[n]\n",
    "#                 else:  # n == 0\n",
    "#                     y[n] = q[n]\n",
    "\n",
    "#         return y\n",
    "\n",
    "#     def generate(self):\n",
    "#         \"\"\"Generate non-stationary time series with varying AR coefficients and time features\"\"\"\n",
    "#         data = np.zeros((self.num_samples, self.seq_length))\n",
    "\n",
    "#         for i in range(self.num_samples):\n",
    "#             # Generate each sample using the AR sequence\n",
    "#             data[i] = self.generate_ar_sequence()\n",
    "\n",
    "#         # Generate time features for each sample\n",
    "#         # For simplicity, we'll use sine and cosine of normalized time index\n",
    "#         time_index = np.arange(self.seq_length) / self.seq_length\n",
    "#         time_features = np.stack([\n",
    "#             np.sin(2 * np.pi * time_index),\n",
    "#             np.cos(2 * np.pi * time_index)\n",
    "#         ], axis=1)  # Shape: (seq_length, 2)\n",
    "#         time_features = np.tile(time_features, (self.num_samples, 1, 1))  # Shape: (num_samples, seq_length, 2)\n",
    "\n",
    "#         return data, time_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SyntheticDataset(Dataset):\n",
    "    def __init__(self, data, time_features, seq_len, label_len, pred_len):\n",
    "        self.data = data  # Shape: (num_samples, seq_length)\n",
    "        self.time_features = time_features  # Shape: (num_samples, seq_length, time_feature_dim)\n",
    "        self.seq_len = seq_len\n",
    "        self.label_len = label_len\n",
    "        self.pred_len = pred_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ts = self.data[idx]  # Shape: (seq_length,)\n",
    "        tf = self.time_features[idx]  # Shape: (seq_length, time_feature_dim)\n",
    "\n",
    "        # Define the indices for encoder and decoder\n",
    "        total_len = self.seq_len + self.pred_len\n",
    "        if len(ts) < total_len:\n",
    "            # Handle cases where the time series is too short\n",
    "            raise IndexError(\"Time series too short for the given sequence length.\")\n",
    "\n",
    "        # Encoder inputs\n",
    "        x_enc = ts[:self.seq_len]\n",
    "        x_mark_enc = tf[:self.seq_len]\n",
    "\n",
    "        # Decoder inputs\n",
    "        x_dec = ts[self.seq_len - self.label_len:self.seq_len + self.pred_len]\n",
    "        x_mark_dec = tf[self.seq_len - self.label_len:self.seq_len + self.pred_len]\n",
    "\n",
    "        # Targets\n",
    "        y = ts[self.seq_len:self.seq_len + self.pred_len]\n",
    "\n",
    "        # Add feature dimension if necessary\n",
    "        x_enc = x_enc.reshape(-1, 1)\n",
    "        x_dec = x_dec.reshape(-1, 1)\n",
    "        y = y.reshape(-1, 1)\n",
    "\n",
    "        # Convert to torch tensors\n",
    "        x_enc = torch.tensor(x_enc, dtype=torch.float32)\n",
    "        x_mark_enc = torch.tensor(x_mark_enc, dtype=torch.float32)\n",
    "        x_dec = torch.tensor(x_dec, dtype=torch.float32)\n",
    "        x_mark_dec = torch.tensor(x_mark_dec, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "        return x_enc, x_mark_enc, x_dec, x_mark_dec, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "from models.timesnet.lora_timesnet import LoRATimesNet\n",
    "\n",
    "\n",
    "class LoRAFineTuner:\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_model,\n",
    "        rank=4,\n",
    "        alpha=1.0,\n",
    "        learning_rate=1e-3,\n",
    "        batch_size=32,\n",
    "        num_epochs=10,\n",
    "    ):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.lora_model = LoRATimesNet(base_model, rank, alpha).to(self.device)\n",
    "        self.optimizer = optim.AdamW(\n",
    "            self.lora_model.get_lora_parameters(), lr=learning_rate\n",
    "        )\n",
    "        self.batch_size = batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "    def train_on_synthetic(self, synthetic_generator, num_samples=1000):\n",
    "        \"\"\"Fine-tune on synthetic data\"\"\"\n",
    "        # Generate synthetic data\n",
    "        synthetic_data, time_features = synthetic_generator.generate()\n",
    "\n",
    "        # Create dataset\n",
    "        dataset = SyntheticDataset(\n",
    "            data=synthetic_data,\n",
    "            time_features=time_features,\n",
    "            seq_len=self.lora_model.base_model.configs.seq_len,\n",
    "            label_len=self.lora_model.base_model.configs.label_len,\n",
    "            pred_len=self.lora_model.base_model.configs.pred_len\n",
    "        )\n",
    "\n",
    "        # Create dataloader\n",
    "        train_loader = DataLoader(\n",
    "            dataset, batch_size=self.batch_size, shuffle=True\n",
    "        )\n",
    "\n",
    "        # Training loop\n",
    "        self.lora_model.train()\n",
    "        for epoch in range(self.num_epochs):\n",
    "            total_loss = 0\n",
    "            for x_enc, x_mark_enc, x_dec, x_mark_dec, y in train_loader:\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                x_enc = x_enc.to(self.device)  # Shape: [batch_size, seq_len, 1]\n",
    "                x_mark_enc = x_mark_enc.to(self.device)\n",
    "                x_dec = x_dec.to(self.device)\n",
    "                x_mark_dec = x_mark_dec.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "\n",
    "                # Permute dimensions to match model expectations\n",
    "                x_enc = x_enc.permute(0, 2, 1)  # Shape: [batch_size, 1, seq_len]\n",
    "                x_dec = x_dec.permute(0, 2, 1)  # Shape: [batch_size, 1, label_len + pred_len]\n",
    "                y = y.permute(0, 2, 1)          # Shape: [batch_size, 1, pred_len]\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = self.lora_model(\n",
    "                    x_enc=x_enc,\n",
    "                    x_mark_enc=x_mark_enc,\n",
    "                    x_dec=x_dec,\n",
    "                    x_mark_dec=x_mark_dec\n",
    "                )\n",
    "                loss = self._compute_loss(outputs, y)\n",
    "\n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            print(f\"Epoch {epoch+1}/{self.num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "\n",
    "    def _compute_loss(self, outputs, targets):\n",
    "        \"\"\"Compute appropriate loss based on task\"\"\"\n",
    "        task_name = self.lora_model.base_model.configs.task_name\n",
    "\n",
    "        if task_name in [\"long_term_forecast\", \"short_term_forecast\"]:\n",
    "            return nn.MSELoss()(outputs, targets)\n",
    "        elif task_name == \"classification\":\n",
    "            return nn.CrossEntropyLoss()(outputs, targets)\n",
    "        else:\n",
    "            return nn.MSELoss()(outputs, targets)\n",
    "\n",
    "    def save_lora_weights(self, path):\n",
    "        \"\"\"Save LoRA weights\"\"\"\n",
    "        lora_state_dict = {\n",
    "            name: param\n",
    "            for name, param in self.lora_model.named_parameters()\n",
    "            if \"lora_\" in name\n",
    "        }\n",
    "        torch.save(lora_state_dict, path)\n",
    "\n",
    "    def load_lora_weights(self, path):\n",
    "        \"\"\"Load LoRA weights\"\"\"\n",
    "        lora_state_dict = torch.load(path)\n",
    "        self.lora_model.load_state_dict(lora_state_dict, strict=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing models...\n",
      "Training on synthetic non-stationary data...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [16, 1, 3], expected input[32, 900, 3] to have 1 channels, but got 900 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 57\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Train on synthetic data\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining on synthetic non-stationary data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 57\u001b[0m train_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mfine_tuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_on_synthetic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Save model and configuration\u001b[39;00m\n\u001b[1;32m     60\u001b[0m save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnonstationary_lora_model.pt\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[0;32mIn[4], line 66\u001b[0m, in \u001b[0;36mLoRAFineTuner.train_on_synthetic\u001b[0;34m(self, synthetic_generator, num_samples)\u001b[0m\n\u001b[1;32m     63\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)          \u001b[38;5;66;03m# Shape: [batch_size, 1, pred_len]\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlora_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_enc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_enc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_mark_enc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_mark_enc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_dec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_dec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_mark_dec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_mark_dec\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_loss(outputs, y)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/LoRA-FT for TS Foundation Models/ft-time-series-fm/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/LoRA-FT for TS Foundation Models/ft-time-series-fm/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/LoRA-FT for TS Foundation Models/experiments/../src/models/timesnet/lora_timesnet.py:85\u001b[0m, in \u001b[0;36mLoRATimesNet.forward\u001b[0;34m(self, x_enc, x_mark_enc, x_dec, x_mark_dec)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_enc, x_mark_enc, x_dec, x_mark_dec):\n\u001b[0;32m---> 85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx_enc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_enc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx_mark_enc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_mark_enc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx_dec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_dec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx_mark_dec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_mark_dec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/LoRA-FT for TS Foundation Models/ft-time-series-fm/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/LoRA-FT for TS Foundation Models/ft-time-series-fm/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/LoRA-FT for TS Foundation Models/experiments/../src/models/timesnet/timesnet.py:151\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_enc, x_mark_enc, x_dec, x_mark_dec, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlong_term_forecast\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshort_term_forecast\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    150\u001b[0m     ):\n\u001b[0;32m--> 151\u001b[0m         dec_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforecast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_enc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_mark_enc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_dec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_mark_dec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m dec_out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpred_len :, :]  \u001b[38;5;66;03m# [B, L, D]\u001b[39;00m\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/LoRA-FT for TS Foundation Models/experiments/../src/models/timesnet/timesnet.py:127\u001b[0m, in \u001b[0;36mModel.forecast\u001b[0;34m(self, x_enc, x_mark_enc, x_dec, x_mark_dec)\u001b[0m\n\u001b[1;32m    124\u001b[0m x_enc \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m stdev\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# embedding\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m enc_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menc_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_enc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_mark_enc\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [B,T,C]\u001b[39;00m\n\u001b[1;32m    128\u001b[0m enc_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_linear(enc_out\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mpermute(\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    130\u001b[0m )  \u001b[38;5;66;03m# align temporal dimension\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# TimesNet\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/LoRA-FT for TS Foundation Models/ft-time-series-fm/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/LoRA-FT for TS Foundation Models/ft-time-series-fm/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/LoRA-FT for TS Foundation Models/experiments/../src/layers/Embed.py:124\u001b[0m, in \u001b[0;36mDataEmbedding.forward\u001b[0;34m(self, x, x_mark)\u001b[0m\n\u001b[1;32m    122\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_embedding(x) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding(x)\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 124\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_embedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemporal_embedding(x_mark) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding(x)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n",
      "File \u001b[0;32m~/Documents/LoRA-FT for TS Foundation Models/ft-time-series-fm/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/LoRA-FT for TS Foundation Models/ft-time-series-fm/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/LoRA-FT for TS Foundation Models/experiments/../src/layers/Embed.py:41\u001b[0m, in \u001b[0;36mTokenEmbedding.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 41\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenConv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Documents/LoRA-FT for TS Foundation Models/ft-time-series-fm/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/LoRA-FT for TS Foundation Models/ft-time-series-fm/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/LoRA-FT for TS Foundation Models/ft-time-series-fm/lib/python3.12/site-packages/torch/nn/modules/conv.py:375\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/LoRA-FT for TS Foundation Models/ft-time-series-fm/lib/python3.12/site-packages/torch/nn/modules/conv.py:359\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_conv_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, weight: Tensor, bias: Optional[Tensor]):\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 359\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m            \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reversed_padding_repeated_twice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_mode\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m            \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_single\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(\n\u001b[1;32m    371\u001b[0m         \u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups\n\u001b[1;32m    372\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [16, 1, 3], expected input[32, 900, 3] to have 1 channels, but got 900 channels instead"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "from models.timesnet.timesnet import Model as TimesNet\n",
    "from models.timesnet.lora_timesnet import LoRATimesNet\n",
    "# from models.timesnet.lora_finetuner import LoRAFineTuner\n",
    "from synth_data_generators.nonstationary import ARNonstationaryGenerator\n",
    "\n",
    "# TimesNet configs\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.seq_len = 900\n",
    "        self.pred_len = 96\n",
    "        self.label_len = 48\n",
    "        self.task_name = 'long_term_forecast'\n",
    "        self.enc_in = 1  # Set to 1 for univariate data\n",
    "        self.dec_in = 1\n",
    "        self.c_out = 1\n",
    "        self.d_model = 16\n",
    "        self.d_ff = 32\n",
    "        self.num_kernels = 6\n",
    "        self.top_k = 5\n",
    "        self.e_layers = 2\n",
    "        self.embed = 'timeF'\n",
    "        self.freq = 'h'\n",
    "        self.dropout = 0.1\n",
    "        \n",
    "# Initialize models\n",
    "print(\"Initializing models...\")\n",
    "configs = Config()\n",
    "base_model = TimesNet(configs)\n",
    "\n",
    "# Initialize fine-tuner\n",
    "fine_tuner = LoRAFineTuner(\n",
    "    base_model=base_model,\n",
    "    rank=4,\n",
    "    alpha=1.0,\n",
    "    learning_rate=1e-3,\n",
    "    batch_size=32,\n",
    "    num_epochs=10\n",
    ")\n",
    "\n",
    "# Create non-stationary data generator\n",
    "generator = ARNonstationaryGenerator(\n",
    "    seq_length=configs.seq_len + configs.pred_len,  # Now 900 + 96 = 996\n",
    "    num_samples=1000,\n",
    "    noise_variance=16\n",
    ")\n",
    "\n",
    "# Train on synthetic data\n",
    "print(\"Training on synthetic non-stationary data...\")\n",
    "train_metrics = fine_tuner.train_on_synthetic(generator)\n",
    "\n",
    "# Save model and configuration\n",
    "save_path = 'nonstationary_lora_model.pt'\n",
    "torch.save({\n",
    "    'model_state_dict': fine_tuner.lora_model.state_dict(),\n",
    "    'train_metrics': train_metrics,\n",
    "    'configs': configs\n",
    "}, save_path)\n",
    "print(f\"Model saved to {save_path}\")\n",
    "\n",
    "# Plot training metrics if available\n",
    "if hasattr(train_metrics, 'loss'):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_metrics['loss'], label='Training Loss')\n",
    "    plt.title('Training Progress')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ft-time-series-fm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
